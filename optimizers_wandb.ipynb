{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gttEcYZych3h"
      },
      "source": [
        "**Installing Wandb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAwmBI8bbJe3"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZSvuJzBc4zd"
      },
      "source": [
        "**Import statements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69z-ajlX05I-"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnSuJiIBcv1O"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl7KYXkRRZBs"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_28PzNuacUyW"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foCbEVjGa8tg"
      },
      "outputs": [],
      "source": [
        "def plotImagesOfEachClass():\n",
        "  #loading the dataset\n",
        "  (trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "  wandb.init(\n",
        "      project=\"Assignment 1\",\n",
        "      entity=\"cs22m006\",\n",
        "      name=\"Assignment1_sample_images\"\n",
        "  )\n",
        "  image_labels = []\n",
        "  images = []\n",
        "  #finding 1 image from each class\n",
        "  for i in range(len(trainX)):\n",
        "    if len(image_labels) == len(class_names):\n",
        "      break\n",
        "    if class_names[trainy[i]] not in image_labels:\n",
        "      image_labels.append(class_names[trainy[i]])\n",
        "      images.append(trainX[i])\n",
        "\n",
        "  #logging 1 image from each class in wandb\n",
        "  wandb.log({\"Sample image for each class \": [wandb.Image(img, caption=caption) for img, caption in zip(images, image_labels)]})\n",
        "\n",
        "plotImagesOfEachClass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rUN0fivd4Yg"
      },
      "source": [
        "# **Question 2 and 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVxycC5wmAo_"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)\n",
        "#loading the dataset\n",
        "(x_train, y_train), (testX, testy) = fashion_mnist.load_data()\n",
        "#flattening the images, originally image is of size 28*28, converting it to 784*1\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
        "x_train = x_train/255.0\n",
        "testX = testX.reshape(testX.shape[0], testX.shape[1]*testX.shape[2])\n",
        "testX = testX/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KVDIokWeG9o"
      },
      "outputs": [],
      "source": [
        "def initializeWeightAndBias(layer_dims, init_mode = \"random_uniform\"):\n",
        "  #layer_dims is a list, which contains number of neurons in each layer\n",
        "  W = []\n",
        "  bias = []\n",
        "  np.random.seed(3)\n",
        "  if(init_mode == \"random_uniform\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.uniform(-0.7, 0.7, (layer_dims[layer_num+1], layer_dims[layer_num])))\n",
        "      bias.append((np.random.uniform(-0.7, 0.7, (layer_dims[layer_num+1],1))))\n",
        "  elif(init_mode == \"xavier\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.randn(layer_dims[layer_num+1],layer_dims[layer_num])*np.sqrt(2/(layer_dims[layer_num+1]+layer_dims[layer_num])))\n",
        "      bias.append(np.random.randn(layer_dims[layer_num+1],1)*np.sqrt(2/(layer_dims[layer_num+1])))\n",
        "  else:  #random normal\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.randn(layer_dims[layer_num+1], layer_dims[layer_num]))\n",
        "      bias.append((np.random.randn(layer_dims[layer_num+1],1)))\n",
        "  return W, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4l-gojV1ctE"
      },
      "outputs": [],
      "source": [
        "def feedForward(W, bias, X, num_hidden_layers, layer_dims, activation_fun = \"tanh\"):\n",
        "  preactivation = []\n",
        "  activation = []\n",
        "  activation.append(X.T)\n",
        "  preactivation.append(X.T)\n",
        "  for i in range(1, num_hidden_layers+1):\n",
        "    preactivation.append(bias[i-1] + np.matmul(W[i-1], activation[(i-1)]))\n",
        "    if(activation_fun == \"sigmoid\"):\n",
        "      activation.append(sigmoid(preactivation[i]))\n",
        "    elif(activation_fun == \"tanh\"):\n",
        "      activation.append(tanh(preactivation[i]))\n",
        "    elif(activation_fun == \"reLU\"):\n",
        "      activation.append(reLU(preactivation[i]))\n",
        "  preactivation.append(bias[-1] + np.dot(W[-1], activation[-1]))\n",
        "  activation.append(softmax(preactivation[-1]))\n",
        "  return activation[-1], activation, preactivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVmdL1b84nT3"
      },
      "outputs": [],
      "source": [
        "def updateParam(W, gradientW, bias, gradientBias, learning_rate):\n",
        "  for i in range(0, len(W)):\n",
        "    W[i] = W[i] - learning_rate*gradientW[i]\n",
        "    bias[i] = bias[i] - learning_rate*gradientBias[i]\n",
        "  return W, bias\n",
        "\n",
        "def updateParamMomentum(W, bias, gradientW, gradientBias, previous_updates_W, previous_updates_Bias, learning_rate, momentum):\n",
        "  for idx in range(len(gradientW)):\n",
        "    previous_updates_W[idx] = momentum*previous_updates_W[idx] + gradientW[idx]\n",
        "    previous_updates_Bias[idx] = momentum*previous_updates_Bias[idx] + gradientBias[idx]\n",
        "  for i in range(0, len(W)):\n",
        "    W[i] = W[i] - learning_rate*gradientW[i]\n",
        "    bias[i] = bias[i] - learning_rate*gradientBias[i]\n",
        "  return W, bias\n",
        "  \n",
        "\n",
        "def updateParamRMS(W, gradientW, bias, gradientBias, learning_rate, v_W, v_bias, beta):\n",
        "  eps = 1e-6\n",
        "  for idx in range(0, len(W)):\n",
        "    v_W_t = beta*v_W[idx] + (1-beta)*np.multiply(gradientW[idx], gradientW[idx])\n",
        "    v_bias_t = beta*v_bias[idx] + (1-beta)*np.multiply(gradientBias[idx], gradientBias[idx])\n",
        "    W[idx] = W[idx] - learning_rate*gradientW[idx]/(np.sqrt(v_W_t)+eps)\n",
        "    bias[idx] = bias[idx] - learning_rate*gradientBias[idx]/(np.sqrt(v_bias_t)+eps)\n",
        "    v_W[idx] = v_W_t\n",
        "    v_bias[idx] = v_bias_t\n",
        "  return W, bias, v_W, v_bias\n",
        "\n",
        "def updateParamAdam(W, bias, gradientW, gradientBias, v_W, v_bias, m_W, m_bias, t, learning_rate, beta1, beta2):\n",
        "\n",
        "  epsilon = 1e-6\n",
        "\n",
        "  for i in range(0, len(W)):\n",
        "    mdW = beta1*m_W[i] + (1-beta1)*gradientW[i]\n",
        "    mdBias = beta1*m_bias[i] + (1-beta1)*gradientBias[i]\n",
        "    vdW = beta2*v_W[i] + (1-beta2)*np.square(gradientW[i])\n",
        "    vdBias = beta2*v_bias[i] + (1-beta2)*np.square(gradientBias[i])\n",
        "    m_w_hat = mdW/(1.0 - beta1**t)\n",
        "    v_w_hat = vdW/(1.0 - beta2**t)\n",
        "    m_bias_hat = mdBias/(1.0 - beta1**t)\n",
        "    v_bias_hat = vdBias/(1.0 - beta2**t)\n",
        "    #adding epsilon to prevent from divide by zero\n",
        "    W[i] = W[i] - (learning_rate * m_w_hat)/np.sqrt(v_w_hat + epsilon)\n",
        "    bias[i] = bias[i] - (learning_rate * m_bias_hat)/np.sqrt(v_bias_hat + epsilon)\n",
        "\n",
        "    v_W[i] = vdW\n",
        "    m_W[i] = mdW\n",
        "    v_bias[i] = vdBias\n",
        "    m_bias[i] = mdBias\n",
        "\n",
        "    return W, bias, v_W, v_bias, m_W, m_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRCs-OZUPD3W"
      },
      "outputs": [],
      "source": [
        "def sigmoid(X):\n",
        "  return 1.0/(1.+np.exp(-X))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def reLU(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def reLU_derivative(x):\n",
        "  return 1*(x>0) \n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(a):\n",
        "  #finding softmax rowwise, since \"a\" is a matrix\n",
        "  return np.exp(a)/np.sum(np.exp(a), axis=0)\n",
        "\n",
        "def softmax_derivative(a):\n",
        "  return softmax(a)*(1-softmax(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meGdgBul6FQJ"
      },
      "outputs": [],
      "source": [
        "def backward_propogation(y_one_hot, x, y, W, bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun, weight_decay, loss_fun):\n",
        "  L = num_hidden_layers+1\n",
        "  gradientPreactivation = []\n",
        "  if(loss_fun == \"cross_entropy\"):\n",
        "    gradientPreactivation.append(activation[L]-y_one_hot)\n",
        "  else:\n",
        "    gradientPreactivation.append((activation[L]-y_one_hot) * softmax_derivative(preactivation[L]))\n",
        "  gradientWeight = []\n",
        "  gradientBias = []\n",
        "  for k in range(L, 0, -1):\n",
        "    gradientWeight.append(np.matmul(gradientPreactivation[-1], activation[k-1].T)/batch_size + (weight_decay*W[k-1])/batch_size)\n",
        "    gradientBias.append(np.sum(gradientPreactivation[-1], axis=1, keepdims=True)/batch_size)\n",
        "    if k==1:\n",
        "      break\n",
        "    if(activation_fun == \"sigmoid\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), sigmoid_derivative(preactivation[k-1])))\n",
        "    elif(activation_fun == \"tanh\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), tanh_derivative(preactivation[k-1])))\n",
        "    if(activation_fun == \"reLU\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), reLU_derivative(preactivation[k-1])))\n",
        "  return gradientWeight[::-1], gradientBias[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvgQ1EXUJgzX"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y, y_hat, W, weight_decay):\n",
        "  loss = 0\n",
        "  for i in range(len(y)):\n",
        "    loss += -1.0*np.sum(y[i]*np.log(y_hat[i]))\n",
        "  #L2 regularization\n",
        "  acc = 0\n",
        "  for i in range(len(W)):\n",
        "    acc += np.sum(W[i]**2)\n",
        "  loss += weight_decay*acc\n",
        "  return loss\n",
        "\n",
        "def mean_squared_error(y, y_hat, W, weight_decay):\n",
        "  loss = 0.5 * np.sum((y-y_hat)**2)\n",
        "  #L2 regularizaation\n",
        "  acc = 0\n",
        "  for i in range(len(W)):\n",
        "    acc += np.sum(W[i]**2)\n",
        "  loss += weight_decay*acc\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOKm03v722fG"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy_and_loss(W, bias, X, y, num_hidden_layers, layer_dims, activation_fun, weight_decay, loss_function, y_one_hot):\n",
        "  hL, _, _ = feedForward(W, bias, X, num_hidden_layers, layer_dims, activation_fun)\n",
        "  #finding predicted class for all the datapoints\n",
        "  predictions = np.argmax(hL, axis = 0)\n",
        "  #counting the elements which has predicted class same as original class\n",
        "  acc = np.sum(y == predictions)/predictions.shape[0]*100\n",
        "  if(loss_function == \"cross_entropy\"):\n",
        "    loss = cross_entropy(y_one_hot, hL, W, weight_decay)\n",
        "  else:\n",
        "    loss = mean_squared_error(y_one_hot, hL, W, weight_decay)\n",
        "  return acc, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZjY9i1d5eTx"
      },
      "outputs": [],
      "source": [
        "def generate_one_hot(n, true_label):\n",
        "  #generating one hot matrix, where all the elements of column i will be set to 0, except the true class index of image i, which will be set to 1\n",
        "  y_one_hot = np.zeros((10, n))\n",
        "  for i in range(n):\n",
        "    y_one_hot[true_label[i]][i] = 1\n",
        "  return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ2rdNHhm7Mm"
      },
      "outputs": [],
      "source": [
        "def plotConfusionMatrix(trainy, y_pred, class_names):\n",
        "  wandb.init(\n",
        "      project=\"Assignment 1\",\n",
        "      entity=\"cs22m006\",\n",
        "      name=\"Confusion matrix\"\n",
        "  )\n",
        "  wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(\n",
        "                          y_true=trainy, preds=y_pred,\n",
        "                          class_names=class_names)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR-dxGTxm_Rj"
      },
      "outputs": [],
      "source": [
        "def calculateTestAccuracy(testX, testy, layer_dims, num_hidden_layers, neurons_in_each_layer, batch_size, W, bias, activation_fun):\n",
        "  batch_count = batch_size\n",
        "  count = 0\n",
        "  for i in range(0, len(testX), batch_size):\n",
        "    #if we are left with lesser data points compared to batch size, still we don't want to ignore those data points\n",
        "    if(i+batch_size>len(testX)):\n",
        "      batch_count = len(testX)-i-1\n",
        "    #calling feed forward to get the prediction class\n",
        "    hL, activation, preactivation = feedForward(W, bias, testX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "    for j in range(i, i+batch_count):\n",
        "      if(np.argmax(hL[:,(j-i)]) == testy[j]):\n",
        "        count+=1\n",
        "  print(\"Accuracy on test data\", (100.0*count)/len(testX))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRtYbi8anHnI"
      },
      "outputs": [],
      "source": [
        "def optimizers():\n",
        "  #setting the default parameters\n",
        "  #change the parameters if you want to test for other parameters as well\n",
        "  config_defaults = {\n",
        "        'epochs': 20,\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 0.1,\n",
        "        'activation_fun': 'tanh',\n",
        "        'optimizer': 'sgd',\n",
        "        'init_mode': 'random_normal',\n",
        "        'weight_decay': 0.05,\n",
        "        'neurons_in_each_layer': 128,\n",
        "        'num_hidden_layers': 4,\n",
        "        'loss_function': 'cross_entropy',\n",
        "        'momentum': 0.9,\n",
        "        'beta': 0.9,\n",
        "        'beta1': 0.9,\n",
        "        'beta2': 0.999\n",
        "    }\n",
        "  wandb.init(project = 'Assignment 1', entity = 'cs22m006', config=config_defaults)\n",
        "  learning_rate = wandb.config.learning_rate\n",
        "  activation_fun = wandb.config.activation_fun\n",
        "  init_mode = wandb.config.init_mode\n",
        "  optimizer = wandb.config.optimizer\n",
        "  batch_size = wandb.config.batch_size\n",
        "  epochs = wandb.config.epochs\n",
        "  weight_decay = wandb.config.weight_decay\n",
        "  neurons_in_each_layer = wandb.config.neurons_in_each_layer\n",
        "  num_hidden_layers = wandb.config.num_hidden_layers\n",
        "  loss_function = wandb.config.loss_function\n",
        "  momentum = wandb.config.momentum\n",
        "  beta = wandb.config.beta\n",
        "  beta1 = wandb.config.beta1\n",
        "  beta2 = wandb.config.beta2\n",
        "\n",
        "  layer_dims = [trainX.shape[1]]\n",
        "  for i in range(num_hidden_layers):\n",
        "    layer_dims.append(neurons_in_each_layer)\n",
        "  layer_dims.append(k)\n",
        "  #initializing weights and biases\n",
        "  W, bias = initializeWeightAndBias(layer_dims, init_mode)\n",
        "\n",
        "  y_one_hot, y_one_hot_val = generate_one_hot(num_images, trainy), generate_one_hot(len(validationy), validationy)\n",
        "  \n",
        "  #initializing variables which is going to used in optimizers\n",
        "  v_W = [0]*(num_hidden_layers+1)\n",
        "  v_bias, m_W, m_bias, gradientW, gradientBias, look_ahead_W, look_ahead_bias, previous_updates_W, previous_updates_Bias = v_W.copy(), v_W.copy(), v_W.copy(), v_W.copy(), v_W.copy(), v_W.copy(), v_W.copy(), v_W.copy(), v_W.copy()\n",
        "  t = 1 #for adam\n",
        "  #setting the run name for wandb\n",
        "  run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}_loss_{}\".format(learning_rate, activation_fun, init_mode, optimizer, batch_size, weight_decay, epochs, neurons_in_each_layer, num_hidden_layers, loss_function)\n",
        "  y_pred = []\n",
        "  for iterationNumber in range(epochs):\n",
        "    loss=0\n",
        "    val_loss = 0\n",
        "    batch_count = batch_size\n",
        "    for i in range(0, num_images, batch_size):\n",
        "      #if we are left with lesser data points compared to batch size, still we don't want to ignore those data points\n",
        "      if(i+batch_size >= num_images):\n",
        "        batch_count = num_images-i\n",
        "\n",
        "      if(optimizer == \"nag\"):\n",
        "        for idx in range(len(W)):\n",
        "          look_ahead_W[idx] = W[idx] - momentum * gradientW[idx]\n",
        "          look_ahead_bias[idx] = bias[idx] - momentum * gradientBias[idx]\n",
        "\n",
        "        hL, activation, preactivation = feedForward(look_ahead_W, look_ahead_bias, trainX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "        gradientW, gradientBias = backward_propogation(y_one_hot[:,i:i+batch_count], trainX[i:i+batch_count], trainy[i:i+batch_count], look_ahead_W, look_ahead_bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun, weight_decay, loss_function)\n",
        "        W, bias = updateParam(W, gradientW, bias, gradientBias, learning_rate)\n",
        "\n",
        "      elif(optimizer == \"nadam\"):\n",
        "        for idx in range(len(W)):\n",
        "          look_ahead_W[idx] = W[idx] - momentum * gradientW[idx]\n",
        "          look_ahead_bias[idx] = bias[idx] - momentum * gradientBias[idx]\n",
        "\n",
        "        hL, activation, preactivation = feedForward(look_ahead_W, look_ahead_bias, trainX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "        gradientW, gradientBias = backward_propogation(y_one_hot[:,i:i+batch_count], trainX[i:i+batch_count], trainy[i:i+batch_count], look_ahead_W, look_ahead_bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun, weight_decay, loss_function)\n",
        "        W, bias, v_W, v_bias, m_W, m_bias = updateParamAdam(W, bias, gradientW, gradientBias, v_W, v_bias, m_W, m_bias, t, learning_rate, beta1, beta2)\n",
        "        t += 1\n",
        "\n",
        "      elif optimizer == 'insert your optimizer here':\n",
        "        #write the update rules and calling of feedforward and backprop here\n",
        "        pass\n",
        "\n",
        "      else:\n",
        "        hL, activation, preactivation = feedForward(W, bias, trainX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "\n",
        "        gradientW, gradientBias = backward_propogation(y_one_hot[:,i:i+batch_count], trainX[i:i+batch_count], trainy[i:i+batch_count], W, bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun, weight_decay, loss_function)\n",
        "  \n",
        "        if(optimizer == \"sgd\"):\n",
        "          W, bias = updateParam(W, gradientW, bias, gradientBias, learning_rate)\n",
        "\n",
        "        elif(optimizer == \"momentum\"):\n",
        "          W, bias = updateParamMomentum(W, bias, gradientW, gradientBias, previous_updates_W, previous_updates_Bias, learning_rate, momentum)\n",
        "        \n",
        "        elif(optimizer == \"rmsprop\"):\n",
        "          W, bias, v_W, v_bias = updateParamRMS(W, gradientW, bias, gradientBias, learning_rate, v_W, v_bias, beta)\n",
        "\n",
        "        elif(optimizer == \"adam\"):\n",
        "          W, bias, v_W, v_bias, m_W, m_bias = updateParamAdam(W, bias, gradientW, gradientBias, v_W, v_bias, m_W, m_bias, t, learning_rate, beta1, beta2)\n",
        "          t += 1\n",
        "      #calculate predicted label for each datapoint in last epoch\n",
        "      if(iterationNumber==epochs-1):\n",
        "        for j in range(i, i+batch_count):\n",
        "          y_pred.append(np.argmax(hL[:,(j-i)]))\n",
        "    train_acc, loss = calculate_accuracy_and_loss(W, bias, trainX, trainy, num_hidden_layers, layer_dims, activation_fun, weight_decay, loss_function, y_one_hot)\n",
        "    val_acc, val_loss = calculate_accuracy_and_loss(W, bias, validationX, validationy, num_hidden_layers, layer_dims, activation_fun, weight_decay, loss_function, y_one_hot_val)\n",
        "    print(\"training_accuracy:\", train_acc, \"validation_accuracy:\", val_acc, \"training_loss:\", loss/(len(trainX)), \"validation loss:\", val_loss/len(validationX), \"epoch:\", iterationNumber)\n",
        "    wandb.log({\"training_accuracy\": train_acc, \"validation_accuracy\": val_acc, \"training_loss\": loss/(len(trainX)), \"validation loss\": val_loss/len(validationX), 'epoch': iterationNumber})\n",
        "  calculateTestAccuracy(testX, testy, layer_dims, num_hidden_layers, neurons_in_each_layer, batch_size, W, bias, activation_fun)\n",
        "  wandb.run.name = run_name\n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn1arCMJTH8C"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  trainX, validationX, trainy, validationy = train_test_split(x_train, y_train, random_state=104, test_size=0.1, shuffle=True)\n",
        "  num_images = len(trainy)\n",
        "  image_size = trainX.shape[1]\n",
        "  y_pred = optimizers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyNrmHlznK0m"
      },
      "outputs": [],
      "source": [
        "#to plot confusion matrix, first run the above cell to get y_pred and then run this cell\n",
        "plotConfusionMatrix(trainy, y_pred, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilYJ-4MnKwgo"
      },
      "outputs": [],
      "source": [
        "# if you want to run the sweep then run this cell\n",
        "trainX, validationX, trainy, validationy = train_test_split(x_train, y_train, random_state=104, test_size=0.1, shuffle=True)\n",
        "num_images = len(trainy)\n",
        "image_size = trainX.shape[1]\n",
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment 1 - Cross Entropy Loss\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"bayes\",\n",
        "  \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.1, 0.01, 1e-3, 1e-4]\n",
        "        },\n",
        "        \"activation_fun\": {\n",
        "            \"values\": [\"sigmoid\", \"tanh\", \"reLU\"]\n",
        "        },\n",
        "        \"init_mode\": {\n",
        "            \"values\": [\"xavier\", \"random_uniform\", \"random_normal\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16,32]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5, 10, 20]\n",
        "        },\n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.0005, 0.05]\n",
        "        },\n",
        "        \"neurons_in_each_layer\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \"num_hidden_layers\": {\n",
        "            \"values\": [3, 4, 5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs22m006\", project=\"Assignment 1\")\n",
        "wandb.agent(sweep_id, optimizers, count = 300)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
